{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis of the Wikipedia graph\n",
    "\n",
    "In the first notebook the extraction of the graph structure from the XML dump of the Wikipedia was described. In this notebook the structured data is used to build a graph representation that is suitable for shortest paths analysis. The spark-graphx library is used for building the Wikipedia graph and running algorithms on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the Spark session\n",
    "\n",
    "Imports and configuration of spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.1.116:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._ // for NotebookSparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@53e41fe9\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`org.apache.spark::spark-graphx:2.4.5`\n",
    "import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.sql._ // for NotebookSparkSession\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .progress(false)\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.local.dir\", \"/data/flachsenberg/tmp/\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "def sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the graph\n",
    "\n",
    "First, the edges determined before are read in and converted into an RDD of suitable format for the spark-graphx library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36medgeDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [from: bigint, to: bigint ... 1 more field]\n",
       "\u001b[36medges\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mInt\u001b[39m]] = MapPartitionsRDD[7] at map at cmd1.sc:2\n",
       "\u001b[36mres1_2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mInt\u001b[39m]] = MapPartitionsRDD[7] at map at cmd1.sc:2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edgeDF = spark.read.load(\"links.parquet\")\n",
    "val edges = edgeDF.rdd.map(row => Edge(row.getLong(0), row.getLong(1), row.getInt(2)))\n",
    "edges.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge(1321276,11135291,1)\n",
      "Edge(1379743,8033245,1)\n",
      "Edge(3896518,8033245,1)\n",
      "Edge(1379751,1676860,1)\n",
      "Edge(1690953,1676860,1)\n",
      "Edge(5102412,1676860,0)\n",
      "Edge(30518,10432444,1)\n",
      "Edge(47047,10432444,1)\n",
      "Edge(1379743,10432444,1)\n",
      "Edge(5556361,10432444,1)\n",
      "Edge count: 68319408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36medgeCount\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m68319408L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.take(10).foreach(println(_))\n",
    "val edgeCount = edges.count()\n",
    "println(s\"Edge count: $edgeCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the nodes determined before are read in and converted into an RDD of suitable format for the spark-graphx library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnodeDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint, title: string]\n",
       "\u001b[36mnodes\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = MapPartitionsRDD[15] at map at cmd3.sc:2\n",
       "\u001b[36mres3_2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = MapPartitionsRDD[15] at map at cmd3.sc:2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nodeDF = spark.read.load(\"titles.parquet\")\n",
    "val nodes = nodeDF.rdd.map(row => (row.getLong(0), row.getString(1)))\n",
    "nodes.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2009958,Zülle)\n",
      "(2009959,Zülicke)\n",
      "(2009960,Festung Germersheim)\n",
      "(2009961,Ludmila Formanová)\n",
      "(2009963,Zübert)\n",
      "(2009964,Zötzsche)\n",
      "(2009965,Ludmila Formanova)\n",
      "(2009966,Zörgiebel)\n",
      "(2009967,Zöpfl)\n",
      "(2009968,Zöpel)\n",
      "Node count: 4017802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnodeCount\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m4017802L\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.take(10).foreach(println(_))\n",
    "val nodeCount = nodes.count()\n",
    "println(s\"Node count: $nodeCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the edge RDD and the node RDD a graph can be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph vertices: 4017802\n",
      "Graph edges: 68319408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@1a181b95\n",
       "\u001b[36mres5_1\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@1a181b95\n",
       "\u001b[36mnofNodes\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m4017802L\u001b[39m\n",
       "\u001b[36mnodeEdges\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m68319408L\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = Graph(nodes, edges, \"NO-ARTICLE\")\n",
    "graph.cache\n",
    "val nofNodes = graph.vertices.count()\n",
    "println(s\"Graph vertices: $nofNodes\")\n",
    "val nodeEdges = graph.edges.count()\n",
    "println(s\"Graph edges: $nodeEdges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default vertices: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnofDefaultNodes\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m0L\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// there should be no default vertices\n",
    "val nofDefaultNodes = graph.vertices.filter({case (_, title) => title == \"NO-ARTICLE\"}).count()\n",
    "println(s\"Default vertices: $nofDefaultNodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest paths analysis\n",
    "\n",
    "The shortest paths in the Wikipedia graph were analyzed. Here, the analysis was performed with the \"Hamburg\" page as the source node. We are interested in the shortest path ending at this vertex, so the graph will be inverted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msourceId\u001b[39m: \u001b[32mVertexId\u001b[39m = \u001b[32m2129L\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (sourceId, _) = graph.vertices.filter(_._2 == \"Hamburg\").first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SSSP problem was solved using the SSSP algorithm given here: https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel-api. The implementation was modified to not only store the weight of the shortest-path but also allow reconstruction (and therefore also stores the parent on the shortest path).\n",
    "\n",
    "While not specified explicitly, this algorithm works similar to the Bellman-Ford algorithm (https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSSSPEntry\u001b[39m\n",
       "\u001b[36mssspentry0\u001b[39m: \u001b[32mSSSPEntry\u001b[39m = \u001b[33mSSSPEntry\u001b[39m(\u001b[32m-1L\u001b[39m, \u001b[32mInfinity\u001b[39m)\n",
       "\u001b[36minitialGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mSSSPEntry\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@48e62c4c"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// one entry in the result graph stores the minimum distance as well as the parent VertexId on the shortest path.\n",
    "case class SSSPEntry (parent: VertexId, dist: Double)\n",
    "// initial entry is infinite distance and no predecessor\n",
    "val ssspentry0 = SSSPEntry(-1, Double.PositiveInfinity)\n",
    "// The initial graph consists of infinity distance except for the source vertex.\n",
    "// Note that we invert the graph because we are interested in paths ending at the given source vertex\n",
    "val initialGraph = graph.reverse.mapVertices((id, _) =>\n",
    "    if (id == sourceId) SSSPEntry(-1, 0.0) else ssspentry0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msssp\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mSSSPEntry\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@4cbe3623\n",
       "\u001b[36mres9_1\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mSSSPEntry\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@4cbe3623"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sssp = initialGraph.pregel(ssspentry0)(\n",
    "  (id, entry, newEntry) => if (newEntry.dist < entry.dist) newEntry else entry,\n",
    "  triplet => {\n",
    "    if (triplet.srcAttr.dist + triplet.attr < triplet.dstAttr.dist) {\n",
    "      Iterator((triplet.dstId, SSSPEntry(triplet.srcId, triplet.srcAttr.dist + triplet.attr)))\n",
    "    } else {\n",
    "      Iterator.empty\n",
    "    }\n",
    "  },\n",
    "  (a, b) => if (a.dist < b.dist) a else b\n",
    ")\n",
    "sssp.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[39m: \u001b[32mList\u001b[39m[(\u001b[32mDouble\u001b[39m, \u001b[32mLong\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m0.0\u001b[39m, \u001b[32m9L\u001b[39m),\n",
       "  (\u001b[32m1.0\u001b[39m, \u001b[32m68310L\u001b[39m),\n",
       "  (\u001b[32m2.0\u001b[39m, \u001b[32m2156603L\u001b[39m),\n",
       "  (\u001b[32m3.0\u001b[39m, \u001b[32m1729296L\u001b[39m),\n",
       "  (\u001b[32m4.0\u001b[39m, \u001b[32m44785L\u001b[39m),\n",
       "  (\u001b[32m5.0\u001b[39m, \u001b[32m288L\u001b[39m),\n",
       "  (\u001b[32m6.0\u001b[39m, \u001b[32m5L\u001b[39m),\n",
       "  (\u001b[32mInfinity\u001b[39m, \u001b[32m18506L\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sssp.vertices.map(_._2.dist).countByValue.toList.sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, from only a minority of nodes the node \"Hamburg\" is not reachable. From every other node, \"Hamburg\" is reachable within a distance of at most 6.\n",
    "\n",
    "The following function allows the reconstruction of the shortest path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgetPathFromTarget\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPathFromTarget(targetId: VertexId, sssp: Graph[SSSPEntry, Int],\n",
    "                      graph: Graph[String, Int]) : Seq[(String, Double)] = {\n",
    "  if (targetId == -1) {\n",
    "    Seq()\n",
    "  }\n",
    "  else {\n",
    "    val (_, entry) = sssp.vertices.filter(_._1 == targetId).first\n",
    "    val (_, title) = graph.vertices.filter(_._1 == targetId).first\n",
    "    Seq((title, entry.dist)) ++ getPathFromTarget(entry.parent, sssp, graph)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List((Zitteraal,2.0), (Zitteraale,2.0), (Carl von Linné,1.0), (Hamburg,0.0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtargetId\u001b[39m: \u001b[32mVertexId\u001b[39m = \u001b[32m10950018L\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (targetId, _) = graph.vertices.filter(_._2 == \"Zitteraal\").first\n",
    "println(getPathFromTarget(targetId, sssp, graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows that the distance from node \"Zitteraal\" (electric eel) to \"Hamburg\" is 2, i.e. needing 2 clicks. The first node \"Zitteraal\" is a redirect onto \"Zitteraale\" which is not counted as a distinct click."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List((Mills Township,6.0), (Mill Township,5.0), (Mill Creek Township,4.0), (Mill Creek Township (Coshocton County, Ohio),3.0), (Township (Vereinigte Staaten),2.0), (Stadt,1.0), (Hamburg,0.0))\n",
      "List((Tenmile,6.0), (Ten Mile,5.0), (Ten Mile River,4.0), (Ten Mile River (Pazifischer Ozean),3.0), (Mendocino County,3.0), (County (Vereinigte Staaten),2.0), (Département,1.0), (Hamburg,0.0))\n",
      "List((Alder Creek,6.0), (North Alder Creek,5.0), (North Fork Alder Creek,4.0), (Mendocino County,3.0), (County (Vereinigte Staaten),2.0), (Département,1.0), (Hamburg,0.0))\n",
      "List((Baker Crossroads,6.0), (Bakers Crossroads,5.0), (Bakers Crossing,4.0), (Bakers,3.0), (Piet Bakers,2.0), (Niederländische Fußballnationalmannschaft,1.0), (Hamburg,0.0))\n",
      "List((Lerdo,6.0), (Lerdo de Tejada,5.0), (Sebastián Lerdo de Tejada (Begriffsklärung),4.0), (Sebastián Lerdo de Tejada,3.0), (Kaiserreich Mexiko (1864–1867),2.0), (Édouard Manet,1.0), (Hamburg,0.0))\n"
     ]
    }
   ],
   "source": [
    "sssp.vertices.filter(_._2.dist == 6.0)\n",
    "             .collect\n",
    "             .map({case (id, _) => id})\n",
    "             .map(x => {\n",
    "                 getPathFromTarget(x, sssp, graph)\n",
    "             }).foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all shortest path with distance 6 are shown. Interestingly, these have a similar structure, i.e. there is a chain of similar sounding articles in the beginning of each path."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
