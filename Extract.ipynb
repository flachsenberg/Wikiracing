{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing of the Wikipedia\n",
    "\n",
    "As the first step the relevant information from the Wikipedia to build a graph of the article relationships is extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the Spark session\n",
    "\n",
    "Imports and configuration specific to the use of the almond kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.1.116:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4fa2a80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`org.apache.spark::spark-graphx:2.4.5`\n",
    "import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "import $ivy.`com.databricks::spark-xml:0.9.0`\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .progress(false)\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.local.dir\", \"/data/flachsenberg/tmp/\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other imports, especially the spark-xml parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.databricks.spark.xml._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.matching.Regex\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import com.databricks.spark.xml._\n",
    "import scala.util.matching.Regex\n",
    "import spark.implicits._\n",
    "\n",
    "def sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML parsing\n",
    "\n",
    "The XML dump of the Wikipedia can be obtained here https://dumps.wikimedia.org/dewiki/ for the German Wikipedia.\n",
    "\n",
    "The Wikipedia XML format is described in detail here: https://meta.wikimedia.org/wiki/Data_dumps/Dump_format\n",
    "\n",
    "The relevant fields in the <code>page</code> are the article identifier <code>id</code>, the namespace <code>ns</code>, the article title <code>title</code>, the redirection <code>redirect</code> (that is non-empty if this article is not an actual article, but a redirection) and finally the <code>revision</code> tag. Here, it should be noted that the redirection is stored as an XML attribute to <code>redirect</code> and the actual text of the article is stored in the <code>text</code> of the <code>revision</code> tag.\n",
    "\n",
    "The relevant information is extracted using the spark-xml reader with the following given schema. This reads the German Wikipedia into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"id\"\u001b[39m, LongType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ns\"\u001b[39m, LongType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"title\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    \u001b[32m\"redirect\"\u001b[39m,\n",
       "    \u001b[33mList\u001b[39m(\u001b[33mStructField\u001b[39m(\u001b[32m\"_title\"\u001b[39m, StringType, true, {})),\n",
       "    true,\n",
       "    {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    \u001b[32m\"revision\"\u001b[39m,\n",
       "    \u001b[33mList\u001b[39m(\u001b[33mStructField\u001b[39m(\u001b[32m\"text\"\u001b[39m, StringType, true, {})),\n",
       "    true,\n",
       "    {}\n",
       "  )\n",
       ")\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint, ns: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = new StructType()\n",
    "      .add(\"id\", LongType)\n",
    "      .add(\"ns\", LongType)\n",
    "      .add(\"title\", StringType)\n",
    "      .add(\"redirect\", new StructType().add(\"_title\", StringType))\n",
    "      .add(\"revision\", new StructType().add(\"text\", StringType))\n",
    "val df = spark.read\n",
    "      .format(\"com.databricks.spark.xml\")\n",
    "      .option(\"rowTag\", \"page\")\n",
    "      .schema(schema)\n",
    "      .xml(\"dewiki-20200601-pages-articles.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame will be filtered and flattened, i.e. only the articles in the main namespace 0 are kept (after that the namespace attribute is no longer needed) and the nested <code>redirect</code> and <code>revision</code> attributes are flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfiltered\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint, title: string ... 2 more fields]\n",
       "\u001b[36mres9_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint, title: string ... 2 more fields]\n",
       "\u001b[36mres9_2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m4017802L\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtered = df.filter($\"ns\" === 0)\n",
    "                 .select(\"id\", \"title\", \"redirect.*\", \"revision.*\")\n",
    "                 .withColumnRenamed(\"_title\", \"redirect\")\n",
    "filtered.cache\n",
    "filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a DataFrame with more than 4 million rows, each corresponding to one article. A DataFrame that contains only the <code>id</code> and the <code>title</code> attribute is derived and stored for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               title|\n",
      "+---+--------------------+\n",
      "|  1|        Alan Smithee|\n",
      "|  3|            Actinium|\n",
      "|  5|             Ang Lee|\n",
      "|  7|Anschluss (Soziol...|\n",
      "|  8|  Anschlussf√§higkeit|\n",
      "| 10|       Aussagenlogik|\n",
      "| 11|          Autopoiese|\n",
      "| 12|                A.A.|\n",
      "| 13| Liste von Autoren/A|\n",
      "| 14| Liste von Autoren/H|\n",
      "| 15| Liste von Autoren/C|\n",
      "| 16| Liste von Autoren/I|\n",
      "| 17| Liste von Autoren/K|\n",
      "| 18| Liste von Autoren/J|\n",
      "| 19| Liste von Autoren/V|\n",
      "| 20| Liste von Autoren/G|\n",
      "| 21| Liste von Autoren/W|\n",
      "| 22| Liste von Autoren/B|\n",
      "| 23| Liste von Autoren/D|\n",
      "| 24| Liste von Autoren/S|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtitles\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint, title: string]\n",
       "\u001b[36mres10_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint, title: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val titles = filtered.drop(\"redirect\").drop(\"text\")\n",
    "titles.cache\n",
    "titles.show()\n",
    "titles.write.save(\"titles.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the edges in the Wikipedia graph will be extracted, i.e. the links between the pages.\n",
    "\n",
    "The link format is described here: https://www.mediawiki.org/wiki/Help:Links\n",
    "\n",
    "Generally, the link is of format <code>\\[\\[link#subsection|display\\]\\]</code>\n",
    "Here, a very simple parser is used. It has a few known problem but will work in most cases. Known limitations are: The <code>&lt;nowiki></code> is ignored, i.e. also the content of explicitly not-to-interpret sections is parsed. Furthermore, links containing the <code>|</code> and <code>#</code> symbol might be problematic.\n",
    "\n",
    "The DataFrame is converted to an RDD and the <code>flatMap</code> function is used to generate for each unique link the tuple <code>(srcId, targetText, weight)</code>; each article might give raise to any number of link entries. Here, the weight is 1 if the link was extracted from the text and 0 if the article is a redirect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpattern\u001b[39m: \u001b[32mRegex\u001b[39m = \\[\\[(?!File:)(?!Datei:)(.+?)\\]\\]\n",
       "\u001b[36mextractedLinks\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = MapPartitionsRDD[56] at flatMap at cmd11.sc:2\n",
       "\u001b[36mres11_2\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Pseudonym\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Regisseur\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Directors Guild of America\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Los Angeles Times\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Internet Movie Database\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Frank Patch \\u2013 Deine Stunden sind gez\\u00e4hlt\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Robert Totten\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Richard Widmark\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Don Siegel\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m\"Manier (Stil)\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pattern = new Regex(\"\\\\[\\\\[(?!File:)(?!Datei:)(.+?)\\\\]\\\\]\")\n",
    "val extractedLinks = filtered.drop(\"title\").rdd.flatMap(row => {\n",
    "    if (row(1) == null) {\n",
    "        pattern.findAllMatchIn(row.getString(2))\n",
    "               .map(_.group(1).split(\"\\\\|\")).filterNot(_.isEmpty)\n",
    "               .map(_(0).split(\"#\")).filterNot(_.isEmpty).map(_(0))\n",
    "               .map((row.getLong(0), _, 1)).toList.distinct\n",
    "    }\n",
    "    else {\n",
    "        Seq((row.getLong(0), row.getString(1), 0))\n",
    "    }\n",
    "})\n",
    "extractedLinks.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RDD already contains the information about the edges in the Wikipedia graph. However, while the source node is already represented as an numerical identifier, the target is still the name of the linked article. The latter must also converted to an identifier - and on the way non-existing articles must be filtered out (\"red links\" in Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number links before: 86619618\n",
      "Number links after: 68319408\n",
      "+-------+--------+------+\n",
      "|   from|      to|weight|\n",
      "+-------+--------+------+\n",
      "|1379866|10729099|     1|\n",
      "|3896518|10729099|     1|\n",
      "|6773765|10729099|     1|\n",
      "|8474492|10729099|     1|\n",
      "|1040895| 5303821|     1|\n",
      "|1379755| 5303821|     1|\n",
      "|5255560| 5303821|     1|\n",
      "| 871998| 2087688|     1|\n",
      "|1244806| 2087688|     1|\n",
      "|1379826| 2087688|     1|\n",
      "| 703022|10370012|     1|\n",
      "|1379866|10370012|     1|\n",
      "|2052128|10370012|     1|\n",
      "|5556361|10370012|     1|\n",
      "| 372995|  284968|     1|\n",
      "|1379840|  284968|     1|\n",
      "| 422289| 7893632|     1|\n",
      "|1379851| 7893632|     1|\n",
      "|7893701| 7893632|     1|\n",
      "|1379775| 1706420|     1|\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlinksWithName\u001b[39m: \u001b[32mDataFrame\u001b[39m = [from: bigint, to: string ... 1 more field]\n",
       "\u001b[36mres19_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [from: bigint, to: string ... 1 more field]\n",
       "\u001b[36mcountBeforeJoin\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m86619618L\u001b[39m\n",
       "\u001b[36mlinks\u001b[39m: \u001b[32mDataFrame\u001b[39m = [from: bigint, to: bigint ... 1 more field]\n",
       "\u001b[36mres19_5\u001b[39m: \u001b[32mDataFrame\u001b[39m = [from: bigint, to: bigint ... 1 more field]\n",
       "\u001b[36mcountAfterJoin\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m68319408L\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// first, create again a DataFrame from the RDD\n",
    "val linksWithName = spark.createDataFrame(extractedLinks)\n",
    "                         .toDF(\"from\", \"to\", \"weight\")\n",
    "linksWithName.cache()\n",
    "val countBeforeJoin = linksWithName.count()\n",
    "println(s\"Number links before: $countBeforeJoin\")\n",
    "// Now perform a join of the title DataFrame with the edges DataFrame\n",
    "// on the titles. That way, if a link target (to) exists as an article\n",
    "// it can be replaced by the corresponding identifier.\n",
    "val links = linksWithName.join(titles, $\"to\" === $\"title\")\n",
    "                          .drop(\"to\").drop(\"title\")\n",
    "                          .withColumnRenamed(\"id\", \"to\")\n",
    "                          .select(\"from\", \"to\", \"weight\") // reorder columns\n",
    "links.cache\n",
    "val countAfterJoin = links.count()\n",
    "println(s\"Number links after: $countAfterJoin\")\n",
    "links.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 87 million links were parsed from the wikipedia, 68 million links point to an article in namespace 0. These will serve as the edges in the Wikipedia graph. It is also stored for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.write.save(\"links.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
